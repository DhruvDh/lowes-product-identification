{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "%pylab inline\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3010 images.\n"
     ]
    }
   ],
   "source": [
    "# Change path to root folder of the data provided\n",
    "path_to_data = '../data/combined/'\n",
    "\n",
    "# a dataset abstraction that figures out classes on it's own based on what folder the images were in\n",
    "_data = torchvision.datasets.DatasetFolder(\n",
    "    path_to_data,                                  # root path, it'll get paths to actual images on it's own\n",
    "    Image.open,                                    # pillow open image function to load data from given path\n",
    "    ['jpg', 'JPG', 'HEIC', 'jpeg', 'jfif'],                                # list of extensions for images in dataset\n",
    "    transform = torchvision.transforms.Compose([   # a pipeline of transformations to apply on the read image\n",
    "#             torchvision.transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "            torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.RandomApply([\n",
    "            torchvision.transforms.RandomHorizontalFlip(p=1.0),\n",
    "            torchvision.transforms.RandomVerticalFlip(p=1.0),\n",
    "            torchvision.transforms.ColorJitter(\n",
    "                brightness=0.7,\n",
    "                contrast=0.7,\n",
    "                saturation=0.7,\n",
    "                hue=0.3),\n",
    "            torchvision.transforms.RandomRotation(15),\n",
    "        ]),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], # the documentation said that alexnet was modelled with\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        # images using this normalisation parameters\n",
    "    ]),\n",
    ")\n",
    "\n",
    "print(\"Found\", str(len(_data)), \"images.\")\n",
    "def _find_classes(dir):\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset.\n",
    "\n",
    "        Args:\n",
    "            dir (string): Root directory path.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "\n",
    "        Ensures:\n",
    "            No class is a subdirectory of another.\n",
    "        \"\"\"\n",
    "        if sys.version_info >= (3, 5):\n",
    "            # Faster and available in Python 3.5 and above\n",
    "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "classes, class_to_idx = _find_classes(path_to_data)\n",
    "idx_to_classes = {v: k for k, v in class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some unfortunate ugly code to split the data randomly into training and testing\n",
    "split_at = 1\n",
    "train, test = torch.utils.data.random_split(\n",
    "    _data,\n",
    "    [int(split_at*(len(_data))), len(_data) - int(split_at*(len(_data)))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 189/189 [01:02<00:00,  3.02it/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "# defining loaders for the train and test datasets\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    train,                                       # dataset \n",
    "    batch_size=batch_size,                       # size of batches to load\n",
    "    shuffle=True,\n",
    "    num_workers=8,                               # no of workers\n",
    "    pin_memory=True                              # Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region.\n",
    ") \n",
    "\n",
    "# test_batches = torch.utils.data.DataLoader(\n",
    "#     test,                                       # dataset \n",
    "#     batch_size=batch_size,                      # size of batches to load\n",
    "#     shuffle=True,\n",
    "#     num_workers=8,                               # no of workers\n",
    "#     pin_memory=True                              # Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region.\n",
    "# ) \n",
    "\n",
    "labels = []\n",
    "for batch in tqdm(train_batches):\n",
    "    for x in batch[1]:\n",
    "        labels.append(x.item())\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "        'balanced', np.unique(labels), np.array(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\models\\squeezenet.py:94: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "  init.kaiming_uniform(m.weight.data)\n",
      "c:\\users\\dhruv\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torchvision\\models\\squeezenet.py:92: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n",
      "  init.normal(m.weight.data, mean=0.0, std=0.01)\n"
     ]
    }
   ],
   "source": [
    "# it'll download alexnet weights on it's own\n",
    "squeezenet = torchvision.models.squeezenet1_1(\n",
    "    pretrained=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FEIT_40W_T8_TUBE_MCRWV_BULB_120V',\n",
       " 'GE_40W_RelaxLED',\n",
       " 'GE_60W_LED_A19_FROST_5000K_8CT',\n",
       " 'GE_Appliance_LED_11W_Soft_White',\n",
       " 'GE_Appliance_LED_40W_Warm_White',\n",
       " 'GE_Basic_LED_60W_Soft_Light',\n",
       " 'GE_Basic_LED_90W_Daylight',\n",
       " 'GE_Classic_LED_65W_Soft_White',\n",
       " 'GE_Vintage_LED_60W_Warm_Light',\n",
       " 'OSI_60W_13W_CFL_SOFT_WHITE_6_CT',\n",
       " 'There_Is_No_Bulb']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of classes from names of folders inside the path to data\n",
    "label_names = classes\n",
    "\n",
    "# set num of classes, so as to set the output dimensions\n",
    "squeezenet.num_classes = len(label_names)\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Dropout(p=0.5)\n",
       "  (1): Conv2d(512, 11, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (2): ReLU(inplace)\n",
       "  (3): AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new instances of the same layers, to get rid of the pretrained weights in the classification layers\n",
    "# we want to learn our own\n",
    "layers = [\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Conv2d(512, squeezenet.num_classes, kernel_size=(1, 1), stride=(1, 1)),\n",
    "#     nn.BatchNorm2d(squeezenet.num_classes),\n",
    "    nn.ReLU(inplace=True),\n",
    "#     nn.Dropout(0.5),\n",
    "    nn.AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
    "]\n",
    "\n",
    "# change the classification layers to the untrained ones we just defined\n",
    "# I am not sure why this syntax works, I found it in a medium article\n",
    "squeezenet.classifier = nn.Sequential(*layers)\n",
    "# but it gets the job done\n",
    "squeezenet.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(\n",
    "    weight = torch.tensor(class_weights).half().to(device)\n",
    ")                # loss function\n",
    "# it's softmax followed by Negative Log Likelihood loss\n",
    "\n",
    "# defining an optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    squeezenet.parameters(),   \n",
    "    lr=1e-4,\n",
    "    amsgrad = True,\n",
    "    eps=1e-4\n",
    ")\n",
    "\n",
    "squeezenet.half()\n",
    "squeezenet.to(device)           # load up all the weights to cuda device memory, if available\n",
    "live_losses = PlotLosses()   # a tool for drawing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:\n",
      "training   (min:    0.039, max:    1.610, cur:    0.053)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(30):\n",
    "    squeezenet.train()  # set model to train mode. not sure what that does, probably some optimisations\n",
    "    \n",
    "    train_errors = []  \n",
    "    test_errors = []\n",
    "    for batch in (tqdm(train_batches)):\n",
    "        X, Y = batch\n",
    "        pred = squeezenet(X.half().to(device))\n",
    "        \n",
    "        optimizer.zero_grad()               # reset the gradients\n",
    "        error = loss(pred, Y.to(device))    # get loss\n",
    "        error.backward()                    # propogate loss backwards\n",
    "        optimizer.step()                    # update params\n",
    "        \n",
    "        train_errors.append(error.data.item())\n",
    "        \n",
    "    \n",
    "    squeezenet.eval()  # set model to evaluate mode. not sure what that does exactly\n",
    "    \n",
    "    # some code to calculate f1 score for the test set\n",
    "    predict = lambda x: torch.argmax(x, dim=1).cpu().numpy()\n",
    "    test_predictions = []\n",
    "    test_actual = []\n",
    "#     for batch in test_batches:\n",
    "#         x, y = batch\n",
    "#         pred = squeezenet(x.half().to(device))\n",
    "#         test_predictions.append(predict(pred))\n",
    "#         test_errors.append(loss(pred, y.to(device)).data.item())    # get loss\n",
    "#         test_actual.append(np.array(y))\n",
    "    \n",
    "#     train_predictions = []\n",
    "#     train_actual = []\n",
    "#     for batch in train_batches:\n",
    "#         x, y = batch\n",
    "#         pred = squeezenet(x.to(device))\n",
    "#         train_predictions.append(predict(pred))\n",
    "#         train_actual.append(np.array(y))\n",
    "\n",
    "    # log data to be plotted later\n",
    "    live_losses.update({\n",
    "        'train_loss': np.mean(np.array(train_errors)),\n",
    "#         'test_loss': np.mean(np.array(test_errors)),\n",
    "#         'train_f1': f1_score(np.concatenate(train_actual), np.concatenate(train_predictions), average='weighted'),\n",
    "#         'test_f1': f1_score(np.concatenate(test_actual), np.concatenate(test_predictions), average='weighted'),\n",
    "    })\n",
    "    live_losses.draw() # plot loss curves   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-272fd87dabd4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m print(classification_report(\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_actual\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# print(classification_report(\n",
    "#     np.concatenate(train_actual),\n",
    "#     np.concatenate(train_predictions),\n",
    "#     labels = range(len(label_names)),\n",
    "#     target_names = label_names\n",
    "# ))\n",
    "\n",
    "\n",
    "print(classification_report(\n",
    "    np.concatenate(test_actual),\n",
    "    np.concatenate(test_predictions),\n",
    "    labels = range(len(label_names)),\n",
    "    target_names = label_names\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "'state_dict': squeezenet.state_dict(),\n",
    "}, \"squeeznet_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = batch[0]\n",
    "# y = batch[1]\n",
    "# alexnet(x)\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(batches)\n",
    "%matplotlib inline\n",
    "def show(img):\n",
    "    plt.figure(figsize=(25,25))\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    \n",
    "show(torchvision.utils.make_grid(x, padding=10, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = iter(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "i = 0\n",
    "for batch in train_batches:\n",
    "    x, y = batch\n",
    "    plt.figure(figsize=(25,25))\n",
    "    npimg = torchvision.utils.make_grid(x, padding=10, normalize=True).numpy()\n",
    "    plt.imsave(str(i)+\".jpg\", np.transpose(npimg, (1,2,0)))\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path to root folder of the data provided\n",
    "path_to_data = '../data/combined/'\n",
    "\n",
    "# a dataset abstraction that figures out classes on it's own based on what folder the images were in\n",
    "_data = torchvision.datasets.DatasetFolder(\n",
    "    path_to_data,                                  # root path, it'll get paths to actual images on it's own\n",
    "    Image.open,                                    # pillow open image function to load data from given path\n",
    "    ['jpg', 'JPG', 'HEIC', 'jpeg', 'jfif'],                                # list of extensions for images in dataset\n",
    "    transform = torchvision.transforms.Compose([   # a pipeline of transformations to apply on the read image\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], # the documentation said that alexnet was modelled with\n",
    "                                 std=[0.229, 0.224, 0.225]),         # images using this normalisation parameters\n",
    "    ]),\n",
    ")\n",
    "\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    _data,                                       # dataset \n",
    "    batch_size=32,                       # size of batches to load\n",
    "    shuffle=True,\n",
    "    num_workers=8,                               # no of workers\n",
    "    pin_memory=True                              # Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region.\n",
    ") \n",
    "\n",
    "squeezenet = torchvision.models.squeezenet1_1()\n",
    "squeezenet.num_classes = len(label_names)\n",
    "# create new instances of the same layers, to get rid of the pretrained weights in the classification layers\n",
    "# we want to learn our own\n",
    "layers = [\n",
    "#     nn.BatchNorm2d(512),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Conv2d(512, squeezenet.num_classes, kernel_size=(1, 1), stride=(1, 1)),\n",
    "#     nn.BatchNorm2d(squeezenet.num_classes),\n",
    "    nn.ReLU(inplace=True),\n",
    "#     nn.Dropout(0.5),\n",
    "    nn.AvgPool2d(kernel_size=13, stride=1, padding=0)\n",
    "]\n",
    "\n",
    "# change the classification layers to the untrained ones we just defined\n",
    "# I am not sure why this syntax works, I found it in a medium article\n",
    "squeezenet.classifier = nn.Sequential(*layers)\n",
    "\n",
    "model = torch.load('../models/squeeznet_half')\n",
    "squeezenet.load_state_dict(model['state_dict'])\n",
    "\n",
    "squeezenet.half()\n",
    "squeezenet.eval()\n",
    "squeezenet.to(device)\n",
    "\n",
    "predict = lambda x: torch.argmax(x, dim=1).cpu().numpy()\n",
    "test_predictions = []\n",
    "test_actual = []\n",
    "for batch in tqdm(train_batches):\n",
    "    x, y = batch\n",
    "    pred = squeezenet(x.half().to(device))\n",
    "    test_predictions.append(predict(pred))\n",
    "    test_actual.append(np.array(y))\n",
    "\n",
    "print(classification_report(\n",
    "    np.concatenate(test_actual),\n",
    "    np.concatenate(test_predictions),\n",
    "    labels = range(len(label_names)),\n",
    "    target_names = label_names\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet.half()\n",
    "squeezenet.eval()\n",
    "predict = lambda x: torch.argmax(x, dim=1).cpu().numpy()\n",
    "test_predictions = []\n",
    "test_actual = []\n",
    "for batch in test_batches:\n",
    "    x, y = batch\n",
    "    pred = squeezenet(x.half().to(device))\n",
    "    test_predictions.append(predict(pred))\n",
    "    test_errors.append(loss(pred, y.to(device)).data.item())    # get loss\n",
    "    test_actual.append(np.array(y))\n",
    "\n",
    "print(classification_report(\n",
    "    np.concatenate(test_actual),\n",
    "    np.concatenate(test_predictions),\n",
    "    labels = range(len(label_names)),\n",
    "    target_names = label_names\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path to root folder of the data provided\n",
    "path_to_data = '../data/combined/'\n",
    "\n",
    "# a dataset abstraction that figures out classes on it's own based on what folder the images were in\n",
    "_data = torchvision.datasets.DatasetFolder(\n",
    "    path_to_data,                                  # root path, it'll get paths to actual images on it's own\n",
    "    Image.open,                                    # pillow open image function to load data from given path\n",
    "    ['jpg', 'JPG', 'HEIC', 'jpeg', 'jfif'],                                # list of extensions for images in dataset\n",
    "    transform = torchvision.transforms.Compose([   # a pipeline of transformations to apply on the read image\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], # the documentation said that alexnet was modelled with\n",
    "                                 std=[0.229, 0.224, 0.225]),         # images using this normalisation parameters\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some unfortunate ugly code to split the data randomly into training and testing\n",
    "split_at = 1\n",
    "train, test = torch.utils.data.random_split(\n",
    "    _data,\n",
    "    [int(split_at*(len(_data))), len(_data) - int(split_at*(len(_data)))]\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "# defining loaders for the train and test datasets\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    train,                                       # dataset \n",
    "    batch_size=batch_size,                       # size of batches to load\n",
    "    shuffle=True,\n",
    "    num_workers=8,                               # no of workers\n",
    "    pin_memory=True                              # Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region.\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for batch in train_batches:\n",
    "    x, y = batch\n",
    "    labels.append(y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight.compute_class_weight(\n",
    "        'balanced', np.unique(labels[0]), labels[0])\n",
    "\n",
    "class_weights = [0.66704036, 1.2192623 , 1.44417476, 1.17125984, 1.31637168,\n",
    "       1.04020979, 1.35227273, 0.59738956]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
