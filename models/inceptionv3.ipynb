{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import os \n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from livelossplot import PlotLosses\n",
    "\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "%pylab inline\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from sklearn.utils import class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3010 images.\n"
     ]
    }
   ],
   "source": [
    "# Change path to root folder of the data provided\n",
    "path_to_data = '../data/combined/'\n",
    "\n",
    "# a dataset abstraction that figures out classes on it's own based on what folder the images were in\n",
    "_data = torchvision.datasets.DatasetFolder(\n",
    "    path_to_data,                                  # root path, it'll get paths to actual images on it's own\n",
    "    Image.open,                                    # pillow open image function to load data from given path\n",
    "    ['jpg', 'JPG', 'HEIC', 'jpeg', 'jfif'],                                # list of extensions for images in dataset\n",
    "    transform = torchvision.transforms.Compose([   # a pipeline of transformations to apply on the read image\n",
    "#             torchvision.transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
    "            torchvision.transforms.Resize((299, 299)),\n",
    "        torchvision.transforms.RandomApply([\n",
    "            torchvision.transforms.RandomHorizontalFlip(p=1.0),\n",
    "            torchvision.transforms.RandomVerticalFlip(p=1.0),\n",
    "            torchvision.transforms.ColorJitter(\n",
    "                brightness=0.7,\n",
    "                contrast=0.7,\n",
    "                saturation=0.7,\n",
    "                hue=0.3),\n",
    "            torchvision.transforms.RandomRotation(15),\n",
    "        ]),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], # the documentation said that alexnet was modelled with\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        # images using this normalisation parameters\n",
    "    ]),\n",
    ")\n",
    "\n",
    "print(\"Found\", str(len(_data)), \"images.\")\n",
    "def _find_classes(dir):\n",
    "        \"\"\"\n",
    "        Finds the class folders in a dataset.\n",
    "\n",
    "        Args:\n",
    "            dir (string): Root directory path.\n",
    "\n",
    "        Returns:\n",
    "            tuple: (classes, class_to_idx) where classes are relative to (dir), and class_to_idx is a dictionary.\n",
    "\n",
    "        Ensures:\n",
    "            No class is a subdirectory of another.\n",
    "        \"\"\"\n",
    "        if sys.version_info >= (3, 5):\n",
    "            # Faster and available in Python 3.5 and above\n",
    "            classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n",
    "        else:\n",
    "            classes = [d for d in os.listdir(dir) if os.path.isdir(os.path.join(dir, d))]\n",
    "        classes.sort()\n",
    "        class_to_idx = {classes[i]: i for i in range(len(classes))}\n",
    "        return classes, class_to_idx\n",
    "\n",
    "classes, class_to_idx = _find_classes(path_to_data)\n",
    "idx_to_classes = {v: k for k, v in class_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some unfortunate ugly code to split the data randomly into training and testing\n",
    "split_at = 1.0\n",
    "train, test = torch.utils.data.random_split(\n",
    "    _data,\n",
    "    [int(split_at*(len(_data))), len(_data) - int(split_at*(len(_data)))]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 189/189 [00:46<00:00,  4.03it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.89423648, 1.78847296, 1.24380165, 2.01203209, 1.44781145,\n",
       "       1.08585859, 1.52869477, 1.60962567, 2.22468588, 1.08156665,\n",
       "       0.26592455])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 16\n",
    "# defining loaders for the train and test datasets\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    train,                                       # dataset \n",
    "    batch_size=batch_size,                       # size of batches to load\n",
    "    shuffle=True,\n",
    "    num_workers=8,                               # no of workers\n",
    "    pin_memory=True                              # Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region.\n",
    ") \n",
    "\n",
    "# test_batches = torch.utils.data.DataLoader(\n",
    "#     test,                                       # dataset \n",
    "#     batch_size=8,                      # size of batches to load\n",
    "#     shuffle=True,\n",
    "#     num_workers=8,                               # no of workers\n",
    "#     pin_memory=True                              # Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region.\n",
    "# )\n",
    "\n",
    "labels = []\n",
    "for batch in tqdm(train_batches):\n",
    "    for x in batch[1]:\n",
    "        labels.append(x.item())\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(\n",
    "        'balanced', np.unique(labels), np.array(labels))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it'll download alexnet weights on it's own\n",
    "squeezenet = torchvision.models.inception_v3(\n",
    "    pretrained=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['FEIT_40W_T8_TUBE_MCRWV_BULB_120V',\n",
       " 'GE_40W_RelaxLED',\n",
       " 'GE_60W_LED_A19_FROST_5000K_8CT',\n",
       " 'GE_Appliance_LED_11W_Soft_White',\n",
       " 'GE_Appliance_LED_40W_Warm_White',\n",
       " 'GE_Basic_LED_60W_Soft_Light',\n",
       " 'GE_Basic_LED_90W_Daylight',\n",
       " 'GE_Classic_LED_65W_Soft_White',\n",
       " 'GE_Vintage_LED_60W_Warm_Light',\n",
       " 'OSI_60W_13W_CFL_SOFT_WHITE_6_CT',\n",
       " 'There_Is_No_Bulb']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of classes from names of folders inside the path to data\n",
    "label_names = classes\n",
    "\n",
    "# set num of classes, so as to set the output dimensions\n",
    "squeezenet.num_classes = len(label_names)\n",
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet.aux_logits = True\n",
    "squeezenet.transform_input = False\n",
    "squeezenet.fc = nn.Linear(2048, squeezenet.num_classes)\n",
    "squeezenet.AuxLogits.fc = nn.Linear(768, squeezenet.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss(\n",
    "    weight = torch.tensor(class_weights).half().to(device)\n",
    ")                # loss function\n",
    "# it's softmax followed by Negative Log Likelihood loss\n",
    "\n",
    "# defining an optimizer\n",
    "optimizer = torch.optim.Adam(\n",
    "    squeezenet.parameters(),   \n",
    "    lr=1e-4,\n",
    "    amsgrad = True,\n",
    "    eps=1e-4\n",
    ")\n",
    "\n",
    "squeezenet.half()\n",
    "squeezenet.to(device)           # load up all the weights to cuda device memory, if available\n",
    "live_losses = PlotLosses()   # a tool for drawing plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss:\n",
      "training   (min:    0.015, max:    0.911, cur:    0.023)\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "for epoch in range(20):\n",
    "    squeezenet.train()  # set model to train mode. not sure what that does, probably some optimisations\n",
    "    \n",
    "    train_errors = []  \n",
    "    test_errors = []\n",
    "    for batch in (tqdm(train_batches)):\n",
    "        X, Y = batch\n",
    "        pred, aux = squeezenet(X.half().to(device))\n",
    "        \n",
    "        optimizer.zero_grad()               # reset the gradients\n",
    "        error = loss(pred, Y.to(device))    # get loss\n",
    "        error_aux = loss(pred, Y.to(device))    # get loss\n",
    "        (error + 0.3*error_aux).backward()                    # propogate loss backwards\n",
    "        optimizer.step()                    # update params\n",
    "        \n",
    "        train_errors.append((error + 0.3 * error_aux).data.item())\n",
    "        \n",
    "    \n",
    "    squeezenet.eval()  # set model to evaluate mode. not sure what that does exactly\n",
    "    \n",
    "    # some code to calculate f1 score for the test set\n",
    "    predict = lambda x: torch.argmax(x, dim=1).cpu().numpy()\n",
    "#     test_predictions = []\n",
    "#     test_actual = []\n",
    "#     for batch in test_batches:\n",
    "#         x, y = batch\n",
    "#         pred = squeezenet(x.half().to(device))\n",
    "#         test_predictions.append(predict(pred))\n",
    "#         test_errors.append(loss(pred, y.to(device)).data.item())    # get loss\n",
    "#         test_actual.append(np.array(y))\n",
    "    \n",
    "#     train_predictions = []\n",
    "#     train_actual = []\n",
    "#     for batch in train_batches:\n",
    "#         x, y = batch\n",
    "#         pred = squeezenet(x.half().to(device))\n",
    "#         train_predictions.append(predict(pred))\n",
    "#         train_actual.append(np.array(y))\n",
    "\n",
    "    # log data to be plotted later\n",
    "    live_losses.update({\n",
    "        'train_loss': np.mean(np.array(train_errors)),\n",
    "#         'test_loss': np.mean(np.array(test_errors)),\n",
    "#         'train_f1': f1_score(np.concatenate(train_actual), np.concatenate(train_predictions), average='weighted'),\n",
    "#         'test_f1': f1_score(np.concatenate(test_actual), np.concatenate(test_predictions), average='weighted'),\n",
    "    })\n",
    "    live_losses.draw() # plot loss curves   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(classification_report(\n",
    "#     np.concatenate(train_actual),\n",
    "#     np.concatenate(train_predictions),\n",
    "#     labels = range(len(label_names)),\n",
    "#     target_names = label_names\n",
    "# ))\n",
    "\n",
    "\n",
    "# print(classification_report(\n",
    "#     np.concatenate(test_actual),\n",
    "#     np.concatenate(test_predictions),\n",
    "#     labels = range(len(label_names)),\n",
    "#     target_names = label_names\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "'state_dict': squeezenet.state_dict(),\n",
    "}, \"inceptionv3_full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "235628544"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# x = batch[0]\n",
    "# y = batch[1]\n",
    "# alexnet(x)\n",
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inception3(\n",
       "  (Conv2d_1a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_2b_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_3b_1x1): BasicConv2d(\n",
       "    (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Conv2d_4a_3x3): BasicConv2d(\n",
       "    (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (Mixed_5b): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5c): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_5d): InceptionA(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch5x5_2): BasicConv2d(\n",
       "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6a): InceptionB(\n",
       "    (branch3x3): BasicConv2d(\n",
       "      (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6b): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6c): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6d): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_6e): InceptionC(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7dbl_5): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (AuxLogits): InceptionAux(\n",
       "    (conv0): BasicConv2d(\n",
       "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (conv1): BasicConv2d(\n",
       "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (fc): Linear(in_features=768, out_features=11, bias=True)\n",
       "  )\n",
       "  (Mixed_7a): InceptionD(\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_2): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_3): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch7x7x3_4): BasicConv2d(\n",
       "      (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7b): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (Mixed_7c): InceptionE(\n",
       "    (branch1x1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3_2b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_1): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_2): BasicConv2d(\n",
       "      (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3a): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch3x3dbl_3b): BasicConv2d(\n",
       "      (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "      (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (branch_pool): BasicConv2d(\n",
       "      (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=2048, out_features=11, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squeezenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batches' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-8f66d91d3ee5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mnpimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'batches' is not defined"
     ]
    }
   ],
   "source": [
    "x, y = next(batches)\n",
    "%matplotlib inline\n",
    "def show(img):\n",
    "    plt.figure(figsize=(25,25))\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')\n",
    "    \n",
    "show(torchvision.utils.make_grid(x, padding=10, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = iter(train_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "i = 0\n",
    "for batch in train_batches:\n",
    "    x, y = batch\n",
    "    plt.figure(figsize=(25,25))\n",
    "    npimg = torchvision.utils.make_grid(x, padding=10, normalize=True).numpy()\n",
    "    plt.imsave(str(i)+\".jpg\", np.transpose(npimg, (1,2,0)))\n",
    "    i = i + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path to root folder of the data provided\n",
    "path_to_data = '../data/combined/'\n",
    "\n",
    "# a dataset abstraction that figures out classes on it's own based on what folder the images were in\n",
    "_data = torchvision.datasets.DatasetFolder(\n",
    "    path_to_data,                                  # root path, it'll get paths to actual images on it's own\n",
    "    Image.open,                                    # pillow open image function to load data from given path\n",
    "    ['jpg', 'JPG', 'HEIC', 'jpeg', 'jfif'],                                # list of extensions for images in dataset\n",
    "    transform = torchvision.transforms.Compose([   # a pipeline of transformations to apply on the read image\n",
    "        torchvision.transforms.Resize((299, 299)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], # the documentation said that alexnet was modelled with\n",
    "                                 std=[0.229, 0.224, 0.225]),         # images using this normalisation parameters\n",
    "    ]),\n",
    ")\n",
    "\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    _data,                                       # dataset \n",
    "    batch_size=32,                       # size of batches to load\n",
    "    shuffle=True,\n",
    "    num_workers=8,                               # no of workers\n",
    "    pin_memory=True                              # Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region.\n",
    ") \n",
    "\n",
    "squeezenet = torchvision.models.inception_v3()\n",
    "squeezenet.num_classes = len(label_names)\n",
    "squeezenet.aux_logits = True\n",
    "squeezenet.fc = nn.Linear(2048, squeezenet.num_classes)\n",
    "squeezenet.AuxLogits.fc = nn.Linear(768, squeezenet.num_classes)\n",
    "\n",
    "model = torch.load('../models/inceptionv3')\n",
    "squeezenet.load_state_dict(model['state_dict'])\n",
    "\n",
    "squeezenet.half()\n",
    "squeezenet.eval()\n",
    "squeezenet.to(device)\n",
    "\n",
    "predict = lambda x: torch.argmax(x, dim=1).cpu().numpy()\n",
    "test_predictions = []\n",
    "test_actual = []\n",
    "for batch in tqdm(train_batches):\n",
    "    x, y = batch\n",
    "    pred = squeezenet(x.half().to(device))\n",
    "    test_predictions.append(predict(pred))\n",
    "    test_actual.append(np.array(y))\n",
    "\n",
    "print(classification_report(\n",
    "    np.concatenate(test_actual),\n",
    "    np.concatenate(test_predictions),\n",
    "    labels = range(len(label_names)),\n",
    "    target_names = label_names\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path to root folder of the data provided\n",
    "path_to_data = '../data/combined/'\n",
    "\n",
    "# a dataset abstraction that figures out classes on it's own based on what folder the images were in\n",
    "_data = torchvision.datasets.DatasetFolder(\n",
    "    path_to_data,                                  # root path, it'll get paths to actual images on it's own\n",
    "    Image.open,                                    # pillow open image function to load data from given path\n",
    "    ['jpg', 'JPG', 'HEIC', 'jpeg', 'jfif'],                                # list of extensions for images in dataset\n",
    "    transform = torchvision.transforms.Compose([   # a pipeline of transformations to apply on the read image\n",
    "        torchvision.transforms.Resize((299, 299)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], # the documentation said that alexnet was modelled with\n",
    "                                 std=[0.229, 0.224, 0.225]),         # images using this normalisation parameters\n",
    "    ]),\n",
    ")\n",
    "\n",
    "squeezenet.half()\n",
    "squeezenet.eval()\n",
    "predict = lambda x: torch.argmax(x, dim=1).cpu().numpy()\n",
    "test_predictions = []\n",
    "test_actual = []\n",
    "for batch in test_batches:\n",
    "    x, y = batch\n",
    "    pred = squeezenet(x.half().to(device))\n",
    "    test_predictions.append(predict(pred))\n",
    "    test_errors.append(loss(pred, y.to(device)).data.item())    # get loss\n",
    "    test_actual.append(np.array(y))\n",
    "\n",
    "print(classification_report(\n",
    "    np.concatenate(test_actual),\n",
    "    np.concatenate(test_predictions),\n",
    "    labels = range(len(label_names)),\n",
    "    target_names = label_names\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change path to root folder of the data provided\n",
    "path_to_data = 'data/'\n",
    "\n",
    "# a dataset abstraction that figures out classes on it's own based on what folder the images were in\n",
    "_data = torchvision.datasets.DatasetFolder(\n",
    "    path_to_data,                                  # root path, it'll get paths to actual images on it's own\n",
    "    Image.open,                                    # pillow open image function to load data from given path\n",
    "    ['jpg', 'JPG'],                                # list of extensions for images in dataset\n",
    "    transform = torchvision.transforms.Compose([   # a pipeline of transformations to apply on the read image\n",
    "        torchvision.transforms.Resize((224, 224)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], # the documentation said that alexnet was modelled with\n",
    "                                 std=[0.229, 0.224, 0.225]),         # images using this normalisation parameters\n",
    "    ]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some unfortunate ugly code to split the data randomly into training and testing\n",
    "split_at = 1\n",
    "train, test = torch.utils.data.random_split(\n",
    "    _data,\n",
    "    [int(split_at*(len(_data))), len(_data) - int(split_at*(len(_data)))]\n",
    ")\n",
    "\n",
    "batch_size = 32\n",
    "# defining loaders for the train and test datasets\n",
    "train_batches = torch.utils.data.DataLoader(\n",
    "    train,                                       # dataset \n",
    "    batch_size=batch_size,                       # size of batches to load\n",
    "    shuffle=True,\n",
    "    num_workers=8,                               # no of workers\n",
    "    pin_memory=True                              # Host to GPU copies are much faster when they originate from pinned (page-locked) memory. CPU tensors and storages expose a pin_memory() method, that returns a copy of the object, with data put in a pinned region.\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squeezenet.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
